{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'prices.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-eefcca71507d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'prices.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mprices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'prices.csv'"
     ]
    }
   ],
   "source": [
    "with open('prices.csv', 'r') as file:\n",
    "    prices = file.read()\n",
    "\n",
    "print(prices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "price_df = pd.read_csv('prices.csv')\n",
    "\n",
    "price_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "price_df.groupby('ticker').median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "price_df.iloc[:16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "open_prices = price_df.pivot(index='date', columns='ticker', values='open')\n",
    "high_prices = price_df.pivot(index='date', columns='ticker', values='high')\n",
    "low_prices = price_df.pivot(index='date', columns='ticker', values='low')\n",
    "close_prices = price_df.pivot(index='date', columns='ticker', values='close')\n",
    "volume = price_df.pivot(index='date', columns='ticker', values='volume')\n",
    "adj_close_prices = price_df.pivot(index='date', columns='ticker', values='adj_close')\n",
    "adj_volume = price_df.pivot(index='date', columns='ticker', values='adj_volume')\n",
    "\n",
    "open_prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "open_prices.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "open_prices.T.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'quiz_tests'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-c46661552357>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mquiz_tests\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcsv_to_close\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv_filepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfield_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \"\"\"Reads in data from a csv file and produces a DataFrame with close data.\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'quiz_tests'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import quiz_tests\n",
    "\n",
    "def csv_to_close(csv_filepath, field_names):\n",
    "    \"\"\"Reads in data from a csv file and produces a DataFrame with close data.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    csv_filepath : str\n",
    "        The name of the csv file to read\n",
    "    field_names : list of str\n",
    "        The field names of the field in the csv file\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    close : DataFrame\n",
    "        Close prices for each ticker and date\n",
    "    \"\"\"\n",
    "    read_csv = pd.read_csv(csv_filepath, names = field_names)\n",
    "    output = read_csv.pivot(index = 'date', columns = 'ticker', values = 'close')\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'quiz_tests'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-0c48e402b025>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mquiz_tests\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdays_to_weeks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen_prices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhigh_prices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlow_prices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclose_prices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \"\"\"Converts daily OHLC prices to weekly OHLC prices.\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'quiz_tests'"
     ]
    }
   ],
   "source": [
    "import quiz_tests\n",
    "\n",
    "\n",
    "def days_to_weeks(open_prices, high_prices, low_prices, close_prices):\n",
    "    \"\"\"Converts daily OHLC prices to weekly OHLC prices.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    open_prices : DataFrame\n",
    "        Daily open prices for each ticker and date\n",
    "    high_prices : DataFrame\n",
    "        Daily high prices for each ticker and date\n",
    "    low_prices : DataFrame\n",
    "        Daily low prices for each ticker and date\n",
    "    close_prices : DataFrame\n",
    "        Daily close prices for each ticker and date\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    open_prices_weekly : DataFrame\n",
    "        Weekly open prices for each ticker and date\n",
    "    high_prices_weekly : DataFrame\n",
    "        Weekly high prices for each ticker and date\n",
    "    low_prices_weekly : DataFrame\n",
    "        Weekly low prices for each ticker and date\n",
    "    close_prices_weekly : DataFrame\n",
    "        Weekly close prices for each ticker and date\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: Implement Function\n",
    "    weekly_open  = open_prices.resample ('W').first()\n",
    "    weekly_high  = high_prices.resample ('W').max()\n",
    "    weekly_low   = low_prices.resample  ('W').min()\n",
    "    weekly_close = close_prices.resample('W').last()\n",
    "    return weekly_open, weekly_high, weekly_low, weekly_close\n",
    "\n",
    "\n",
    "quiz_tests.test_days_to_weeks(days_to_weeks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'quiz_tests'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-95561cb80411>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mquiz_tests\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcalculate_returns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \"\"\"\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'quiz_tests'"
     ]
    }
   ],
   "source": [
    "import quiz_tests\n",
    "\n",
    "\n",
    "def calculate_returns(close):\n",
    "    \"\"\"\n",
    "    Compute returns for each ticker and date in close.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    close : DataFrame\n",
    "        Close prices for each ticker and date\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    returns : DataFrame\n",
    "        Returns for each ticker and date\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    # [Pt - P(t-1)] / P(t-1) = returns\n",
    "    # close price subtracts by the close price of the previous day, \n",
    "    # divides the substraction by the close price of the previous day\n",
    "    \n",
    "    curr_close_price = close.shift(0)\n",
    "    prev_close_price = close.shift(1)\n",
    "    returns          = (curr_close_price - prev_close_price) / prev_close_price\n",
    "    \n",
    "    return returns\n",
    "\n",
    "\n",
    "quiz_tests.test_calculate_returns(calculate_returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import project_tests\n",
    "import numpy as np\n",
    "\n",
    "def generate_positions(prices):\n",
    "    \"\"\"\n",
    "    Generate the following signals:\n",
    "     - Long 30 share of stock when the price is above 50 dollars\n",
    "     - Short 10 shares when it's below 20 dollars\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    prices : DataFrame\n",
    "        Prices for each ticker and date\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    final_positions : DataFrame\n",
    "        Final positions for each ticker and date\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    signal_long  = prices > 50\n",
    "    signal_short = prices < 20\n",
    "    signal_long = signal_long.astype(np.int)\n",
    "    signal_short = signal_short.astype(np.int)\n",
    "    long_positions  = signal_long  * 30\n",
    "    short_positions = signal_short * (-10)\n",
    "    final_positions = long_positions + short_positions\n",
    "    #=================================================#\n",
    "#     prices = prices.astype(np.int)\n",
    "#     if prices > 50:\n",
    "#         long = prices * 30\n",
    "#     elif prices < 20:\n",
    "#         short = prices * (-10)\n",
    "#     final_positions = long + short\n",
    "    return final_positions\n",
    "\n",
    "\n",
    "project_tests.test_generate_positions(generate_positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'project_tests'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-081fae52e184>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mproject_tests\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdate_top_industries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_n\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \"\"\"\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'project_tests'"
     ]
    }
   ],
   "source": [
    "import project_tests\n",
    "\n",
    "\n",
    "def date_top_industries(prices, sector, date, top_n):\n",
    "    \"\"\"\n",
    "    Get the set of the top industries for the date\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    prices : DataFrame\n",
    "        Prices for each ticker and date\n",
    "    sector : Series\n",
    "        Sector name for each ticker\n",
    "    date : Date\n",
    "        Date to get the top performers\n",
    "    top_n : int\n",
    "        Number of top performers to get\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    top_industries : set\n",
    "        Top industries for the date\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "\n",
    "    top_industries = sector.loc[prices.loc[date].nlargest(top_n).index]\n",
    "    return set(top_industries)\n",
    "\n",
    "\n",
    "project_tests.test_date_top_industries(date_top_industries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "def analyze_returns(net_returns):\n",
    "    \"\"\"\n",
    "    Perform a t-test, with the null hypothesis being that the mean return is zero.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    net_returns : Pandas Series\n",
    "        A Pandas Series for each date\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    t_value\n",
    "        t-statistic from t-test\n",
    "    p_value\n",
    "        Corresponding p-value\n",
    "    \"\"\"\n",
    "    # TODO: Perform one-tailed t-test on net_returns\n",
    "    # Hint: You can use stats.ttest_1samp() to perform the test.\n",
    "    #       However, this performs a two-tailed t-test.\n",
    "    #       You'll need to divde the p-value by 2 to get the results of a one-tailed p-value.\n",
    "    null_hypothesis = 0.0\n",
    "    # mean\n",
    "    mean = pd.Series.mean(net_returns)\n",
    "    # standard deviation\n",
    "    std  = pd.Series.std (net_returns)\n",
    "    # t-statistics\n",
    "    t, p = stats.ttest_1samp(net_returns, null_hypothesis)\n",
    "    \n",
    "    return t, p/2\n",
    "    \n",
    "def test_run(filename='net_returns.csv'):\n",
    "    \"\"\"Test run analyze_returns() with net strategy returns from a file.\"\"\"\n",
    "    net_returns = pd.Series.from_csv(filename, header=0)\n",
    "    t, p = analyze_returns(net_returns)\n",
    "    print(\"t-statistic: {:.3f}\\np-value: {:.6f}\".format(t, p))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    test_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Visualize the distribution of different samples.\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_histogram(sample, title, bins=16, **kwargs):\n",
    "    \"\"\"Plot the histogram of a given sample of random values.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    sample : pandas.Series\n",
    "        raw values to build histogram\n",
    "    title : str\n",
    "        plot title/header\n",
    "    bins : int\n",
    "        number of bins in the histogram\n",
    "    kwargs : dict \n",
    "        any other keyword arguments for plotting (optional)\n",
    "    \"\"\"\n",
    "    # TODO: Plot histogram\n",
    "    plt.hist(sample, bins=bins)\n",
    "    plt.title(title)\n",
    "    # TODO: show the plot\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "\n",
    "def test_run():\n",
    "    \"\"\"Test run plot_histogram() with different samples.\"\"\"\n",
    "    # Load and plot histograms of each sample\n",
    "    # Note: Try plotting them one by one if it's taking too long\n",
    "    A = pd.read_csv(\"A.csv\", header=None, squeeze=True)\n",
    "    plot_histogram(A, title=\"Sample A\")\n",
    "    \n",
    "    B = pd.read_csv(\"B.csv\", header=None, squeeze=True)\n",
    "    plot_histogram(B, title=\"Sample B\")\n",
    "    \n",
    "    C = pd.read_csv(\"C.csv\", header=None, squeeze=True)\n",
    "    plot_histogram(C, title=\"Sample C\")\n",
    "    \n",
    "    D = pd.read_csv(\"D.csv\", header=None, squeeze=True)\n",
    "    plot_histogram(D, title=\"Sample D\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    test_run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def is_normal_ks(sample, test=stats.kstest, p_level=0.05, **kwargs):\n",
    "    \"\"\"\n",
    "    sample: a sample distribution\n",
    "    test: a function that tests for normality\n",
    "    p_level: if the test returns a p-value > than p_level, assume normality\n",
    "    \n",
    "    return: True if distribution is normal, False otherwise\n",
    "    \"\"\"\n",
    "    mean = sample.mean()\n",
    "    stdd = sample.std()\n",
    "    normal_args = (mean, stdd)\n",
    "    \n",
    "    t_stat, p_value = test(sample, 'norm', normal_args, **kwargs)\n",
    "    print(\"Test statistic: {}, p-value: {}\".format(t_stat, p_value))\n",
    "    print(\"Is the distribution Likely Normal? {}\".format(p_value > p_level))\n",
    "    return p_value > p_level\n",
    "\n",
    "quiz_tests.test_is_normal_ks(is_normal_ks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def regression_slope_and_intercept(xSeries, ySeries):\n",
    "    \"\"\"\n",
    "    xSeries: pandas series, x variable\n",
    "    ySeries: pandas series, y variable\n",
    "    \"\"\"\n",
    "    lr = LinearRegression()\n",
    "    #TODO: get the values from each series, reshape to be 2 dimensional\n",
    "    #set s1 to the x variable, s2 to the y variable\n",
    "    xVar = xSeries.values.reshape(-1,1)\n",
    "    yVar = ySeries.values.reshape(-1,1)\n",
    "    \n",
    "    #TODO: call LinearRegression.fit().  Pass in the x variable then y variable\n",
    "    lr.fit(xVar, yVar)\n",
    "    #TODO: obtain the slope and intercept\n",
    "    slope = lr.coef_[0][0]\n",
    "    intercept = lr.intercept_[0]\n",
    "    \n",
    "    return (slope, intercept)\n",
    "\n",
    "quiz_tests.test_regression_slope_and_intercept(regression_slope_and_intercept);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fit_arima(lret):\n",
    "    \n",
    "    #TODO: choose autoregression lag of 1\n",
    "    AR_lag_p = 1\n",
    "    p = AR_lag_p\n",
    "    #TODO: choose moving average lag of 1\n",
    "    MA_lag_q = 1\n",
    "    q = MA_lag_q\n",
    "    #TODO: choose order of integration 1\n",
    "    order_of_integration_d = 1\n",
    "    d = order_of_integration_d\n",
    "    #TODO: Create a tuple of p,d,q\n",
    "    order = (p, d, q)\n",
    "    \n",
    "    #TODO: create an ARIMA model object, passing in the values of the lret pandas series,\n",
    "    # and the tuple containing the (p,d,q) order arguments\n",
    "    arima_model = ARIMA(lret.values, order=order)\n",
    "    \n",
    "    arima_result = arima_model.fit()\n",
    "    \n",
    "    #TODO: from the result of calling ARIMA.fit(),\n",
    "    # save and return the fitted values, autoregression parameters, and moving average parameters\n",
    "    fittedvalues = arima_result.fittedvalues\n",
    "    arparams = arima_result.arparams\n",
    "    maparams = arima_result.maparams\n",
    "   \n",
    "    return fittedvalues,arparams,maparams\n",
    "\n",
    "quiz_tests.test_fit_arima(fit_arima)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def get_most_volatile(prices):\n",
    "    \"\"\"Return the ticker symbol for the most volatile stock.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    prices : pandas.DataFrame\n",
    "        a pandas.DataFrame object with columns: ['ticker', 'date', 'price']\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    ticker : string\n",
    "        ticker symbol for the most volatile stock\n",
    "    \"\"\"\n",
    "    # TODO: Fill in this function.\n",
    "    prices_list = []\n",
    "    prices = prices.set_index('date')\n",
    "    for ticker in prices.ticker.unique():\n",
    "        ticker_price = prices[prices['ticker'] == ticker]['price'] \n",
    "        ticker_lret  = np.log(ticker_price/ticker_price.shift(1))\n",
    "        ticker_std_ = ticker_lret.std() \n",
    "        print (\"Standard deviation of ticker \" + ticker + \": \", ticker_std_)\n",
    "        prices_list.append(ticker_std_)\n",
    "    prices_list = pd.Series(prices_list)\n",
    "    prices_list.index = prices.ticker.unique()\n",
    "    ticker = prices_list.idxmax()\n",
    "\n",
    "    for ticker in prices.ticker:\n",
    "        price_A_ = prices[prices['ticker'] == 'A']['price']\n",
    "        lret_A_  = np.log(price_A_/price_A_.shift(1))\n",
    "        ticker_A_ = lret_A_.std() \n",
    "        price_B_ = prices[prices['ticker'] == 'B']['price']\n",
    "        lret_B_  = np.log(price_B_/price_B_.shift(1))\n",
    "        ticker_B_ = lret_B_.std() \n",
    "    print (\"Standard deviation of ticker A: \", ticker_A_)\n",
    "    print (\"Standard deviation of ticker B: \", ticker_B_)\n",
    "    \n",
    "    return ticker\n",
    "    \n",
    "    # if ticker_A_ < ticker_B_:\n",
    "    #     return 'B'\n",
    "    # return 'A'\n",
    "    \n",
    "def test_run(filename='prices.csv'):\n",
    "    \"\"\"Test run get_most_volatile() with stock prices from a file.\"\"\"\n",
    "    prices = pd.read_csv(filename, parse_dates=['date'])\n",
    "    print(\"Most volatile stock: {}\".format(get_most_volatile(prices)))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    test_run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import quiz_tests\n",
    "\n",
    "def calculate_simple_moving_average(rolling_window, close):\n",
    "    \"\"\"\n",
    "    Compute the simple moving average.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    rolling_window: int\n",
    "        Rolling window length\n",
    "    close : DataFrame\n",
    "        Close prices for each ticker and date\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    simple_moving_average : DataFrame\n",
    "        Simple moving average for each ticker and date\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    simple_moving_average = close.rolling(window = rolling_window).mean()\n",
    "    return simple_moving_average\n",
    "\n",
    "\n",
    "quiz_tests.test_calculate_simple_moving_average(calculate_simple_moving_average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def estimate_volatility(prices, l):\n",
    "    \"\"\"Create an exponential moving average model of the volatility of a stock\n",
    "    price, and return the most recent (last) volatility estimate.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    prices : pandas.Series\n",
    "        A series of adjusted closing prices for a stock.\n",
    "        \n",
    "    l : float\n",
    "        The 'lambda' parameter of the exponential moving average model. Making\n",
    "        this value smaller will cause the model to weight older terms less \n",
    "        relative to more recent terms.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    last_vol : float\n",
    "        The last element of your exponential moving averge volatility model series.\n",
    "    \n",
    "    \"\"\"\n",
    "    # TODO: Implement the exponential moving average volatility model and return the last value.\n",
    "    # alpha = 1 - lambda\\\n",
    "    curr_prices = prices          # prices.shift(0)\n",
    "    prev_prices = prices.shift(1) # shift prices to 1 day before\n",
    "    log_returns = np.log(curr_prices) - np.log(prev_prices)\n",
    "    \n",
    "    # last_vol = np.sqrt((log_returns**2).ewm(alpha=1-l).mean().iloc[-1])\n",
    "    volatility = (log_returns**2).ewm(alpha=1-l).mean()\n",
    "    most_recent_volatility = np.sqrt(volatility.iloc[-1])\n",
    "    return most_recent_volatility\n",
    "\n",
    "def test_run(filename='data.csv'):\n",
    "    \"\"\"Test run get_most_volatile() with stock prices from a file.\"\"\"\n",
    "    prices = pd.read_csv(filename, parse_dates=['date'], index_col='date', squeeze=True)\n",
    "    print(\"Most recent volatility estimate: {:.6f}\".format(estimate_volatility(prices, 0.7)))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    test_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def is_spread_stationary(spread, p_level=0.05):\n",
    "    \"\"\"\n",
    "    spread: obtained from linear combination of two series with a hedge ratio\n",
    "    \n",
    "    p_level: level of significance required to reject null hypothesis of non-stationarity\n",
    "    \n",
    "    returns:\n",
    "        True if spread can be considered stationary\n",
    "        False otherwise\n",
    "    \"\"\"\n",
    "    #TODO: use the adfuller function to check the spread\n",
    "    #adf_result = \n",
    "    adf_result = adfuller(spread)\n",
    "    #get the p-value\n",
    "    #pvalue = \n",
    "    pvalue = adf_result[1]\n",
    "    print(f\"pvalue {pvalue:.4f}\")\n",
    "    if pvalue <= p_level:\n",
    "        print(f\"pvalue is <= {p_level}, assume spread is stationary\")\n",
    "        return True\n",
    "    else:\n",
    "        print(f\"pvalue is > {p_level}, assume spread is not stationary\")\n",
    "        return False\n",
    "    \n",
    "quiz_tests.test_is_spread_stationary(is_spread_stationary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz: Arithmetic Rate of Return\n",
    "\n",
    "Now, let's use cumprod() and cumsum() to calculate average rate of return.  \n",
    "\n",
    "For consistency, let's assume the rate of return is calculated as $ \\frac{P_t}{P_t - 1} - 1 $\n",
    "\n",
    "### Arithmetic Rate of Return:\n",
    "\n",
    "$ \\frac{1}{n} \\sum_{i=1}^{n} r_i = \\frac{1}{n}(r_1 + r_2 + r_3 + r_4 + ... + r_n) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import quiz_tests\n",
    "\n",
    "def calculate_arithmetic_rate_of_return(close):\n",
    "    \"\"\"\n",
    "    Compute returns for each ticker and date in close.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    close : DataFrame\n",
    "        Close prices for each ticker and date\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    arithmnetic_returns : Series\n",
    "        arithmnetic_returns at the end of the period for each ticker\n",
    "        \n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    returns = close / close.shift(1) - 1\n",
    "    arithmetic_returns = returns.cumsum(axis=0).iloc[returns.shape[0]-1]/returns.shape[0] \n",
    "    return arithmetic_returns\n",
    "\n",
    "\n",
    "quiz_tests.test_calculate_arithmetic_rate_of_return(calculate_arithmetic_rate_of_return)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hints\n",
    "\n",
    "### covariance matrix\n",
    "If we have $m$ stock series, the covariance matrix is an $m \\times m$ matrix containing the covariance between each pair of stocks.  We can use [numpy.cov](https://docs.scipy.org/doc/numpy/reference/generated/numpy.cov.html) to get the covariance.  We give it a 2D array in which each row is a stock series, and each column is an observation at the same period of time.\n",
    "\n",
    "The covariance matrix $\\mathbf{P} = \n",
    "\\begin{bmatrix}\n",
    "\\sigma^2_{1,1} & ... & \\sigma^2_{1,m} \\\\ \n",
    "... & ... & ...\\\\\n",
    "\\sigma_{m,1} & ... & \\sigma^2_{m,m}  \\\\\n",
    "\\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def covariance_matrix(returns):\n",
    "    \"\"\"\n",
    "    Create a function that takes the return series of a set of stocks\n",
    "    and calculates the covariance matrix.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    returns : numpy.ndarray\n",
    "        2D array containing stock return series in each row.\n",
    "                \n",
    "    Returns\n",
    "    -------\n",
    "    x : np.ndarray\n",
    "        A numpy ndarray containing the covariance matrix\n",
    "    \"\"\"\n",
    "    \n",
    "    #covariance matrix of returns\n",
    "    #cov = \n",
    "    cov = np.cov(returns)    \n",
    "    return cov\n",
    "\n",
    "quiz_tests.test_covariance_matrix(covariance_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization with cvxpy\n",
    "\n",
    "http://www.cvxpy.org/\n",
    "\n",
    "Practice using cvxpy to solve a simple optimization problem. Find the optimal weights on a two-asset portfolio given the variance of Stock A, the variance of Stock B, and the correlation between Stocks A and B. Create a function that takes in these values as arguments and returns the vector of optimal weights, i.e., \n",
    "\n",
    "$\\mathbf{x} = \\begin{bmatrix}\n",
    "x_A & x_B\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "\n",
    "Remember that the constraint in this problem is: $x_A + x_B = 1$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hints\n",
    "\n",
    "### standard deviation\n",
    "standard deviation $\\sigma_A = \\sqrt(\\sigma^2_A)$, where $\\sigma^2_A$ is variance of $x_A$\n",
    "look at `np.sqrt()`\n",
    "\n",
    "### covariance\n",
    "correlation between the stocks is $\\rho_{A,B}$\n",
    "\n",
    "covariance between the stocks is $\\sigma_{A,B} = \\sigma_A \\times \\sigma_B \\times \\rho_{A,B}$\n",
    "\n",
    "### x vector\n",
    "create a vector of 2 x variables $\\mathbf{x} = \\begin{bmatrix}\n",
    "x_A & x_B\n",
    "\\end{bmatrix}\n",
    "$\n",
    "we can use `cvx.Variable(2)`\n",
    "\n",
    "### covariance matrix\n",
    "The covariance matrix $P = \n",
    "\\begin{bmatrix}\n",
    "\\sigma^2_A & \\sigma_{A,B} \\\\ \n",
    "\\sigma_{A,B} & \\sigma^2_B \n",
    "\\end{bmatrix}$\n",
    "\n",
    "We can create a 2 x 2 matrix using a 2-dimensional numpy array\n",
    "`np.array([[\"Cindy\", \"Liz\"],[\"Eddy\", \"Brok\"]])`\n",
    "\n",
    "### quadratic form\n",
    "We can write the portfolio variance $\\sigma^2_p = \\mathbf{x^T} \\mathbf{P} \\mathbf{x}$\n",
    "\n",
    "Recall that the $\\mathbf{x^T} \\mathbf{P} \\mathbf{x}$ is called the quadratic form.\n",
    "We can use the cvxpy function `quad_form(x,P)` to get the quadratic form.\n",
    "\n",
    "### objective function\n",
    "Next, we want to define the objective function.  In this case, we want to minimize something.  What do we want to minimize in this case?  We want to minimize the portfolio variance, which is defined by our quadratic form $\\mathbf{x^T} \\mathbf{P} \\mathbf{x}$\n",
    "\n",
    "We can find the objective function using cvxpy `objective = cvx.Minimize()`.  Can you guess what to pass into this function?\n",
    "\n",
    "\n",
    "### constraints\n",
    "We can also define our constraints in a list.  For example, if you wanted the $\\sum_{1}^{n}x = 1$, you could save a variable as `[sum(x)==1]`, where x was created using `cvx.Variable()`.\n",
    "\n",
    "### optimization\n",
    "So now that we have our objective function and constraints, we can solve for the values of $\\mathbf{x}$.\n",
    "cvxpy has the constructor `Problem(objective, constraints)`, which returns a `Problem` object.\n",
    "\n",
    "The `Problem` object has a function solve(), which returns the minimum of the solution.  In this case, this is the minimum variance of the portfolio.\n",
    "\n",
    "It also updates the vector $\\mathbf{x}$.\n",
    "\n",
    "We can check out the values of $x_A$ and $x_B$ that gave the minimum portfolio variance by using `x.value`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cvxpy as cvx\n",
    "import numpy as np\n",
    "\n",
    "def optimize_twoasset_portfolio(varA, varB, rAB):\n",
    "    \"\"\"Create a function that takes in the variance of Stock A, the variance of\n",
    "    Stock B, and the correlation between Stocks A and B as arguments and returns \n",
    "    the vector of optimal weights\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    varA : float\n",
    "        The variance of Stock A.\n",
    "        \n",
    "    varB : float\n",
    "        The variance of Stock B.    \n",
    "        \n",
    "    rAB : float\n",
    "        The correlation between Stocks A and B.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    x : np.ndarray\n",
    "        A 2-element numpy ndarray containing the weights on Stocks A and B,\n",
    "        [x_A, x_B], that minimize the portfolio variance.\n",
    "    \n",
    "    \"\"\"\n",
    "    # TODO: Use cvxpy to determine the weights on the assets in a 2-asset\n",
    "    # portfolio that minimize portfolio variance.\n",
    "    std = np.sqrt(varA)\n",
    "    #cov = \n",
    "    # correlation between the stocks is ðœŒð´,ðµ\n",
    "    # covariance between the stocks is ðœŽð´,ðµ=ðœŽð´Ã—ðœŽðµÃ—ðœŒð´,ðµ\n",
    "    cov = np.sqrt(varA) * np.sqrt(varB) * rAB\n",
    "    # x = \n",
    "    x = cvx.Variable(2)\n",
    "    # P = \n",
    "    P = np.array([[varA, cov],[cov, varB]])\n",
    "    #objective = \n",
    "    objective = cvx.Minimize(cvx.quad_form(x,P))\n",
    "    # constraints = \n",
    "    constraints = [sum(x)==1]\n",
    "    # problem = \n",
    "    problem = cvx.Problem(objective, constraints)\n",
    "    #min_value = \n",
    "    min_value = problem.solve()\n",
    "    # xA,xB = \n",
    "    xA,xB = x.value\n",
    "    # return xA and xB\n",
    "    return (xA, xB)\n",
    "\n",
    "quiz_tests.test_optimize_twoasset_portfolio(optimize_twoasset_portfolio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's our objective?\n",
    "http://www.cvxpy.org/\n",
    "\n",
    "Let's see how we can use optimization to meet a more advanced objective.  We want to both minimize the portfolio variance and also want to closely track a market cap weighted index.  In other words, we're trying to minimize the distance between the weights of our portfolio and the weights of the index.\n",
    "\n",
    "$Minimize \\left [ \\sigma^2_p + \\lambda \\sqrt{\\sum_{1}^{m}(weight_i - indexWeight_i)^2} \\right  ]$ where $m$ is the number of stocks in the portfolio, and $\\lambda$ is a scaling factor that you can choose.\n",
    "## Hints\n",
    "\n",
    "### x vector\n",
    "To create a vector of M variables $\\mathbf{x} = \\begin{bmatrix}\n",
    "x_1 &...& x_M\n",
    "\\end{bmatrix}\n",
    "$\n",
    "we can use `cvx.Variable(m)`\n",
    "\n",
    "### covariance matrix\n",
    "If we have $m$ stock series, the covariance matrix is an $m \\times m$ matrix containing the covariance between each pair of stocks.  We can use [numpy.cov](https://docs.scipy.org/doc/numpy/reference/generated/numpy.cov.html) to get the covariance.  We give it a 2D array in which each row is a stock series, and each column is an observation at the same period of time.\n",
    "\n",
    "The covariance matrix $\\mathbf{P} = \n",
    "\\begin{bmatrix}\n",
    "\\sigma^2_{1,1} & ... & \\sigma^2_{1,m} \\\\ \n",
    "... & ... & ...\\\\\n",
    "\\sigma_{m,1} & ... & \\sigma^2_{m,m}  \\\\\n",
    "\\end{bmatrix}$\n",
    "### portfolio variance\n",
    "We can write the portfolio variance $\\sigma^2_p = \\mathbf{x^T} \\mathbf{P} \\mathbf{x}$\n",
    "\n",
    "Recall that the $\\mathbf{x^T} \\mathbf{P} \\mathbf{x}$ is called the quadratic form.\n",
    "We can use the cvxpy function `quad_form(x,P)` to get the quadratic form.\n",
    "\n",
    "### Distance from index weights\n",
    "We want portfolio weights that track the index closely.  So we want to minimize the distance between them.\n",
    "Recall from the Pythagorean theorem that you can get the distance between two points in an x,y plane by adding the square of the x and y distances and taking the square root.  Extending this to any number of dimensions is called the L2 norm.  So: $\\sqrt{\\sum_{1}^{n}(weight_i - indexWeight_i)^2}$  Can also be written as $\\left \\| \\mathbf{x} - \\mathbf{index} \\right \\|_2$.  There's a cvxpy function called [norm()](https://www.cvxpy.org/api_reference/cvxpy.atoms.other_atoms.html#norm)\n",
    "`norm(x, p=2, axis=None)`.  The default is already set to find an L2 norm, so you would pass in one argument, which is the difference between your portfolio weights and the index weights.\n",
    "\n",
    "### objective function\n",
    "We want to minimize both the portfolio variance and the distance of the portfolio weights from the index weights.\n",
    "We also want to choose a `scale` constant, which is $\\lambda$ in the expression. This lets us choose how much priority we give to minimizing the difference from the index, relative to minimizing the variance of the portfolio.  If you choose a higher value for `scale` ($\\lambda$), do you think this gives more priority to minimizing the difference, or minimizing the variance?\n",
    "\n",
    "We can find the objective function using cvxpy `objective = cvx.Minimize()`.  Can you guess what to pass into this function?\n",
    "\n",
    "### constraints\n",
    "We can also define our constraints in a list.  For example, you'd want the weights to sum to one. So $\\sum_{1}^{n}x = 1$.  You may also need to go long only, which means no shorting, so no negative weights.  So $x_i >0 $ for all $i$. you could save a variable as `[x >= 0, sum(x) == 1]`, where x was created using `cvx.Variable()`.\n",
    "### optimization\n",
    "So now that we have our objective function and constraints, we can solve for the values of $\\mathbf{x}$.\n",
    "cvxpy has the constructor `Problem(objective, constraints)`, which returns a `Problem` object.\n",
    "\n",
    "The `Problem` object has a function solve(), which returns the minimum of the solution.  In this case, this is the minimum variance of the portfolio.\n",
    "\n",
    "It also updates the vector $\\mathbf{x}$.\n",
    "\n",
    "We can check out the values of $x_A$ and $x_B$ that gave the minimum portfolio variance by using `x.value`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'cvxpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-0e02eaf2726d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mcvxpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcvx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0moptimize_portfolio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreturns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m.00001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \"\"\"\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'cvxpy'"
     ]
    }
   ],
   "source": [
    "import cvxpy as cvx\n",
    "import numpy as np\n",
    "\n",
    "def optimize_portfolio(returns, index_weights, scale=.00001):\n",
    "    \"\"\"\n",
    "    Create a function that takes the return series of a set of stocks, the index weights,\n",
    "    and scaling factor. The function will minimize a combination of the portfolio variance\n",
    "    and the distance of its weights from the index weights.  \n",
    "    The optimization will be constrained to be long only, and the weights should sum to one.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    returns : numpy.ndarray\n",
    "        2D array containing stock return series in each row.\n",
    "        \n",
    "    index_weights : numpy.ndarray\n",
    "        1D numpy array containing weights of the index.\n",
    "        \n",
    "    scale : float\n",
    "        The scaling factor applied to the distance between portfolio and index weights\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    x : np.ndarray\n",
    "        A numpy ndarray containing the weights of the stocks in the optimized portfolio\n",
    "    \"\"\"\n",
    "    # TODO: Use cvxpy to determine the weights on the assets\n",
    "    # that minimizes the combination of portfolio variance and distance from index weights\n",
    "    \n",
    "    # number of stocks m is number of rows of returns, and also number of index weights\n",
    "    #m = \n",
    "    m = returns.shape[0]\n",
    "    # m = len(index_weights)\n",
    "    #covariance matrix of returns\n",
    "    #cov = \n",
    "    cov = np.cov(returns)\n",
    "    # x variables (to be found with optimization)\n",
    "    #x = \n",
    "    x = cvx.Variable(m) \n",
    "    #portfolio variance, in quadratic form\n",
    "    #portfolio_variance = \n",
    "    portfolio_variance = cvx.quad_form(x, cov)\n",
    "    # euclidean distance (L2 norm) between portfolio and index weights\n",
    "    #distance_to_index = \n",
    "    distance_to_index = cvx.norm(x - index_weights)\n",
    "    #objective function\n",
    "    #objective = \n",
    "    objective = cvx.Minimize(portfolio_variance + scale * distance_to_index)\n",
    "    #constraints\n",
    "    #constraints = \n",
    "    constraints = [x >= 0, sum(x) == 1]\n",
    "    #use cvxpy to solve the objective\n",
    "    problem = cvx.Problem(objective, constraints).solve()\n",
    "    #retrieve the weights of the optimized portfolio\n",
    "    #x_values\n",
    "    x_values = x.value\n",
    "    return x_values\n",
    "\n",
    "quiz_tests_advanced.test_optimize_portfolio(optimize_portfolio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Count words.\"\"\"\n",
    "import string\n",
    "\n",
    "def count_words(text):\n",
    "    \"\"\"Count how many times each unique word occurs in text.\"\"\"\n",
    "    counts = dict()  # dictionary of { <word>: <count> } pairs to return\n",
    "    # TODO: Convert to lowercase\n",
    "    lower_case_text = text.lower()\n",
    "    # TODO: Split text into tokens (words), leaving out punctuation\n",
    "    line = lower_case_text.translate(lower_case_text.maketrans(\"\", \"\", string.punctuation))\n",
    "    # (Hint: Use regex to split on non-alphanumeric characters)\n",
    "    words = line.split(\" \")\n",
    "    # words = re.split('[a-zA-Z]', lower_case_text)\n",
    "    # TODO: Aggregate word counts using a dictionary\n",
    "    for word in words:\n",
    "        if word in counts:\n",
    "            counts[word] += 1\n",
    "        else:\n",
    "            counts[word]  = 1\n",
    "    return counts\n",
    "    \n",
    "def test_run():\n",
    "    with open(\"input.txt\", \"r\") as f:\n",
    "        text = f.read()\n",
    "        counts = count_words(text)\n",
    "        sorted_counts = sorted(counts.items(), key=lambda pair: pair[1], reverse=True)\n",
    "        \n",
    "        print(\"10 most common words:\\nWord\\tCount\")\n",
    "        for word, count in sorted_counts[:10]:\n",
    "            print(\"{}\\t{}\".format(word, count))\n",
    "        \n",
    "        print(\"\\n10 least common words:\\nWord\\tCount\")\n",
    "        for word, count in sorted_counts[-10:]:\n",
    "            print(\"{}\\t{}\".format(word, count))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Setting the random seed, feel free to change it and see different solutions.\n",
    "np.random.seed(42)\n",
    "\n",
    "def stepFunction(t):\n",
    "    if t >= 0:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "def prediction(X, W, b):\n",
    "    return stepFunction((np.matmul(X,W)+b)[0])\n",
    "\n",
    "# TODO: Fill in the code below to implement the perceptron trick.\n",
    "# The function should receive as inputs the data X, the labels y,\n",
    "# the weights W (as an array), and the bias b,\n",
    "# update the weights and bias W, b, according to the perceptron algorithm,\n",
    "# and return W and b.\n",
    "def perceptronStep(X, y, W, b, learn_rate = 0.01):\n",
    "    # Fill in code\n",
    "    for x in range(len(X)):\n",
    "        y_hat = prediction(X[x], W, b)\n",
    "        if y[x] - y_hat ==  1:\n",
    "            W[0] += X[x][0] * learn_rate\n",
    "            W[1] += X[x][1] * learn_rate\n",
    "            b += learn_rate\n",
    "        if y[x] - y_hat == -1:\n",
    "            W[0] -= X[x][0] * learn_rate\n",
    "            W[1] -= X[x][1] * learn_rate\n",
    "            b -= learn_rate            \n",
    "    return W, b\n",
    "    \n",
    "# This function runs the perceptron algorithm repeatedly on the dataset,\n",
    "# and returns a few of the boundary lines obtained in the iterations,\n",
    "# for plotting purposes.\n",
    "# Feel free to play with the learning rate and the num_epochs,\n",
    "# and see your results plotted below.\n",
    "def trainPerceptronAlgorithm(X, y, learn_rate = 0.01, num_epochs = 25):\n",
    "    x_min, x_max = min(X.T[0]), max(X.T[0])\n",
    "    y_min, y_max = min(X.T[1]), max(X.T[1])\n",
    "    W = np.array(np.random.rand(2,1))\n",
    "    b = np.random.rand(1)[0] + x_max\n",
    "    # These are the solution lines that get plotted below.\n",
    "    boundary_lines = []\n",
    "    for i in range(num_epochs):\n",
    "        # In each epoch, we apply the perceptron step.\n",
    "        W, b = perceptronStep(X, y, W, b, learn_rate)\n",
    "        boundary_lines.append((-W[0]/W[1], -b/W[1]))\n",
    "    return boundary_lines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Write a function that takes as input a list of numbers, and returns\n",
    "# the list of values given by the softmax function.\n",
    "def softmax(L):\n",
    "    # values = list()\n",
    "    # for i in range(len(L)):\n",
    "    #     values.append(np.exp(L[i]) / np.sum(np.exp(L)))\n",
    "    return [np.exp(L[i]) / np.sum(np.exp(L)) for i in range(len(L))]\n",
    "    \n",
    "    # expL = np.exp(L)\n",
    "    # sumExpL = sum(expL)\n",
    "    # result = []\n",
    "    # for i in expL:\n",
    "    #     result.append(i*1.0/sumExpL)\n",
    "    # return result\n",
    "    \n",
    "    # Note: The function np.divide can also be used here, as follows:\n",
    "    # def softmax(L):\n",
    "    #     expL = np.exp(L)\n",
    "    #     return np.divide (expL, expL.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Write a function that takes as input two lists Y, P,\n",
    "# and returns the float corresponding to their cross-entropy.\n",
    "def cross_entropy(Y, P):\n",
    "    assert (len(Y) == len(P))\n",
    "    # CE = list()\n",
    "    # for i in range (len(P)):\n",
    "    #     CE.append(Y[i] * np.log(P[i]) + (1 - Y[i]) * np.log(1 - P[i]))\n",
    "    # return -np.sum(CE)\n",
    "    return -np.sum([Y[i]*np.log(P[i]) + (1 - Y[i]) * np.log(1 - P[i]) for i in range(len(P))])\n",
    "    \n",
    "    # Y = np.float_(Y)\n",
    "    # print(Y)\n",
    "    # P = np.float_(P)\n",
    "    # print(P)\n",
    "    # return -np.sum(Y * np.log(P) + (1 - Y) * np.log(1 - P))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
